# -*- coding: utf-8 -*-
"""Tubes PCD

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12NtZSL5JXwHSRFyPDq0TN0eB-pExokAg

# **Import File**
"""

import zipfile
import os
from google.colab import drive

# 0. Extract ZIP File

zip_path = '/content/drive/MyDrive/TB.zip'  # Path to your ZIP file
extract_path = '/content/TB'  # Path where files will be extracted
train_dir = os.path.join(extract_path, 'TB/train')

# Mount Google Drive to access the ZIP file
drive.mount('/content/drive')

# Check if the dataset is already extracted
if not os.path.exists(extract_path):
    os.makedirs(extract_path)

if zipfile.is_zipfile(zip_path):
    print(f"Extracting {zip_path}â€¦")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("Extraction complete.")
else:
    print(f"{zip_path} is not a valid ZIP file.")

# 1. Dataset Preparation
train_dir = os.path.join(extract_path, 'TB/train')
val_dir = os.path.join(extract_path, 'TB/val')
test_dir = os.path.join(extract_path, 'TB/test')

"""# **VIT TRANSFORMER + CLASSIFIER**"""

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.preprocessing import image

# Data Preparation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_test_datagen = ImageDataGenerator(rescale=1.0/255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

# Define MLP-Mixer
input_layer = Input(shape=(224, 224, 3))

# Flatten image patches
x = tf.keras.layers.Reshape((224 * 224, 3))(input_layer)

# Apply MLP on spatial dimension
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)

# Apply MLP on channel dimension
x = tf.keras.layers.Permute((2, 1))(x)  # Swap dimensions for channel mixing
x = Dense(64, activation='relu')(x)
x = Dense(32, activation='relu')(x)

# Flatten and add final classifier layers
x = Flatten()(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=input_layer, outputs=output)

# Compile and Train
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator
)

# Prediction and Display
def display_predictions(generator, model, num_images=10):
    # Get a batch of images and labels from the generator
    images, labels = next(generator)

    # Predict the class of each image in the batch
    predictions = model.predict(images)

    # Set up the plot
    plt.figure(figsize=(15, 10))

    for i in range(num_images):
        # Get the image and the actual label
        img = images[i]
        label = labels[i]
        pred = predictions[i]

        # Convert prediction to 0 or 1
        pred_label = 1 if pred > 0.5 else 0

        # Plot the image
        plt.subplot(2, 5, i + 1)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"True: {label[0]} | Pred: {pred_label}")

    plt.tight_layout()
    plt.show()

# Display 10 predictions
display_predictions(val_generator, model, num_images=10)

"""# **MLP-Mixer**"""

from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import numpy as np

# EarlyStopping Callback
early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor the validation loss
    patience=5,          # Stop training after 5 epochs without improvement
    restore_best_weights=True  # Restore the best weights
)

# Train the model with EarlyStopping
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator,
    callbacks=[early_stopping]  # Include the callback
)

# Visualize Training History
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss')
plt.show()

# Display 10 Images with Predictions and True Labels
def display_predictions(generator, model, num_images=10):
    # Get a batch of images and labels from the generator
    images, labels = next(generator)

    # Predict the class probabilities
    predictions = model.predict(images)

    # Convert probabilities to binary predictions
    predictions = (predictions > 0.5).astype(int).flatten()

    # Plot images with predictions
    plt.figure(figsize=(15, 10))
    for i in range(num_images):
        plt.subplot(2, 5, i + 1)
        plt.imshow(images[i])
        true_label = 'Benign' if labels[i] == 0 else 'Malicious'
        predicted_label = 'Benign' if predictions[i] == 0 else 'Malicious'
        plt.title(f"True: {true_label}\nPred: {predicted_label}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Display 10 images from the validation set with predictions and labels
display_predictions(val_generator, model, num_images=10)

"""# **Capsule Network**"""

from tensorflow.keras.layers import Input, Conv2D, Reshape, Dense, Layer
from tensorflow.keras.models import Model
import tensorflow as tf

class CapsuleLayer(Layer):
    def __init__(self, num_capsules, dim_capsule, **kwargs):
        super(CapsuleLayer, self).__init__(**kwargs)
        self.num_capsules = num_capsules
        self.dim_capsule = dim_capsule

    def build(self, input_shape):
        self.W = self.add_weight(shape=[self.num_capsules, input_shape[-1], self.dim_capsule],
                                 initializer="glorot_uniform",
                                 trainable=True)

    def call(self, inputs):
        u_hat = tf.einsum('bij,kjl->bikl', inputs, self.W)
        return tf.reduce_sum(u_hat, axis=1)

# Input Layer
input_layer = Input(shape=(224, 224, 3))

# Convolution Layer for Feature Extraction
conv1 = Conv2D(64, kernel_size=9, strides=2, activation='relu')(input_layer)

# Reshape to Capsules
reshaped = Reshape((64, -1))(conv1)

# Capsule Layer
caps_layer = CapsuleLayer(num_capsules=10, dim_capsule=16)(reshaped)

# Output Layer
output = Dense(1, activation='sigmoid')(caps_layer)

# Define Model
model = Model(inputs=input_layer, outputs=output)

# Compile and Train
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator
)